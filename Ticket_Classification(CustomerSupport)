!pip install fasttext
import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import fasttext
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.utils import resample
import nltk
import seaborn as sns
import matplotlib.pyplot as plt

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')

# Set up stopwords and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Load the dataset
file_path = '/content/customer_support_tickets.csv'  # Replace with your dataset file path
df = pd.read_csv(file_path)
# Display the first few rows of the dataset for inspection
print("Initial Dataset Sample:")
print(df.head())

# Count the occurrences of each Ticket Type
ticket_type_counts = df['Ticket Type'].value_counts()
print("\nTicket Type Counts at the Beginning:")
print(ticket_type_counts)
print("-" * 50)

# Select relevant columns
df = df[['Ticket Type', 'Ticket Description']]

# Preprocess text
def preprocess_text(text):
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = text.lower()  # Convert to lowercase
    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords
    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())  # Lemmatize words
    return text

df['Ticket Description'] = df['Ticket Description'].fillna('').apply(preprocess_text)

# Balance classes (oversample minority classes)
majority_class = df['Ticket Type'].value_counts().idxmax()
df_majority = df[df['Ticket Type'] == majority_class]
df_minority = df[df['Ticket Type'] != majority_class]

# Upsample minority classes
df_minority_upsampled = resample(df_minority,
                                 replace=True,     # Sample with replacement
                                 n_samples=len(df_majority),    # Match majority class size
                                 random_state=42)  # Reproducible results

# Combine majority and upsampled minority classes
df = pd.concat([df_majority, df_minority_upsampled])

# Display sample dataset after preprocessing
print("\nSample Dataset After Preprocessing and Balancing:")
print(df[['Ticket Type', 'Ticket Description']].head())
# Split the dataset into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

# Save training data in FastText format
with open('train.txt', 'w') as f:
    for _, row in train.iterrows():
        f.write(f"__label__{row['Ticket Type']} {row['Ticket Description']}\n")

# Save test data for evaluation
with open('test.txt', 'w') as f:
    for _, row in test.iterrows():
        f.write(f"__label__{row['Ticket Type']} {row['Ticket Description']}\n")

# Show distribution of Ticket Types in train and test datasets
print("\nTrain Dataset Ticket Type Distribution:")
print(train['Ticket Type'].value_counts())

print("\nTest Dataset Ticket Type Distribution:")
print(test['Ticket Type'].value_counts())
model = fasttext.train_supervised(
    input="train.txt",
    epoch=100,  # Increase training epochs
    lr=0.3,     # Adjust learning rate
    wordNgrams=4,  # Experiment with higher n-grams
    bucket=300000,  # Increase hash size for unique n-grams
    dim=150,  # Use higher embedding dimensions
    loss='ova'
)

print(train['Ticket Type'].value_counts())

# Save the trained FastText model to a .bin file
model_file_path = "fasttext_ticket_classifier.bin"
model.save_model(model_file_path)
print(f"Model saved to {model_file_path}")

# Evaluate the model on the test set
result = model.test("test.txt")

# Display results
print("\nFastText Model Performance:")
print(f"Number of samples: {result[0]}")
print(f"Precision (Accuracy): {result[1] * 100:.2f}%")
print(f"Recall: {result[2] * 100:.2f}%")

# Detailed evaluation using sklearn
y_true = []
y_pred = []

with open("test.txt", "r") as f:
    for line in f:
        parts = line.split()
        label = parts[0].replace("__label__", "")
        text = " ".join(parts[1:])
        y_true.append(label)
        prediction = model.predict(text)
        predicted_label = prediction[0][0].replace("__label__", "")
        y_pred.append(predicted_label)

# Calculate and display detailed metrics
print("\nDetailed Classification Report:")
print(classification_report(y_true, y_pred))
print(f"Overall Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%")


